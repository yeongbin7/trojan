{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNro6TwxqYnJ82ymZ+iqhnu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nh7WkAVV05m6","executionInfo":{"status":"ok","timestamp":1681354226751,"user_tz":-540,"elapsed":20738,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"07336f70-80d4-40e3-a8b5-cda1363b5563"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import IPython\n","from google.colab import output\n","\n","display(IPython.display.Javascript('''\n"," function ClickConnect(){\n","   btn = document.querySelector(\"colab-connect-button\")\n","   if (btn != null){\n","     console.log(\"Click colab-connect-button\"); \n","     btn.click() \n","     }\n","   \n","   btn = document.getElementById('ok')\n","   if (btn != null){\n","     console.log(\"Click reconnect\"); \n","     btn.click() \n","     }\n","  }\n","  \n","setInterval(ClickConnect,60000)\n","'''))\n","\n","print(\"Done.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"7qpopa612FMc","executionInfo":{"status":"ok","timestamp":1681354237343,"user_tz":-540,"elapsed":674,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"103de9e1-4c27-4440-f728-7783ef026009"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n"," function ClickConnect(){\n","   btn = document.querySelector(\"colab-connect-button\")\n","   if (btn != null){\n","     console.log(\"Click colab-connect-button\"); \n","     btn.click() \n","     }\n","   \n","   btn = document.getElementById('ok')\n","   if (btn != null){\n","     console.log(\"Click reconnect\"); \n","     btn.click() \n","     }\n","  }\n","  \n","setInterval(ClickConnect,60000)\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Done.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"Mbj3GlZM2G6S","executionInfo":{"status":"ok","timestamp":1681354245009,"user_tz":-540,"elapsed":2340,"user":{"displayName":"김민","userId":"01899801562476156168"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["x_train = pd.read_hdf('/content/drive/MyDrive/aes/t100/x_train.h5').values\n","x_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HFnrXYC2ojQ","executionInfo":{"status":"ok","timestamp":1681354404190,"user_tz":-540,"elapsed":140866,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"6a7dbb47-1b97-4cab-f28e-e037f5b0e3d3"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.159101  ,  0.172359  ,  0.16352   , ...,  0.        ,\n","         0.00883892,  0.        ],\n","       [ 0.0331459 ,  0.0331459 ,  0.0331459 , ...,  0.0132584 ,\n","         0.0198876 ,  0.0110486 ],\n","       [ 0.159101  ,  0.159101  ,  0.185617  , ..., -0.00883892,\n","        -0.00883892,  0.0132584 ],\n","       ...,\n","       [ 0.159101  ,  0.150262  ,  0.16352   , ...,  0.00441946,\n","         0.        ,  0.        ],\n","       [ 0.024307  ,  0.0220973 ,  0.0265168 , ...,  0.0198876 ,\n","         0.0265168 ,  0.0220973 ],\n","       [ 0.0397751 ,  0.0441946 ,  0.0441946 , ...,  0.00441946,\n","         0.00883892,  0.        ]], dtype=float32)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["x_test = pd.read_hdf('/content/drive/MyDrive/aes/t100/x_test.h5').values\n","x_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3M76pMJ2tOB","executionInfo":{"status":"ok","timestamp":1681354467070,"user_tz":-540,"elapsed":62889,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"73dda350-b6c7-426e-ca6f-d48f53540ee8"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.024307  ,  0.0265168 ,  0.0309362 , ...,  0.00220973,\n","         0.00441946,  0.        ],\n","       [ 0.00883892,  0.00662919,  0.00441946, ...,  0.00883892,\n","         0.00662919,  0.0132584 ],\n","       [ 0.0198876 ,  0.0309362 ,  0.0265168 , ...,  0.        ,\n","         0.        ,  0.        ],\n","       ...,\n","       [ 0.0309362 ,  0.0309362 ,  0.024307  , ..., -0.00220973,\n","        -0.00220973, -0.00441946],\n","       [ 0.0309362 ,  0.0287265 ,  0.0397751 , ...,  0.0220973 ,\n","         0.0198876 ,  0.0220973 ],\n","       [ 0.0220973 ,  0.0220973 ,  0.024307  , ...,  0.024307  ,\n","         0.0154681 ,  0.0110486 ]], dtype=float32)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["y_train = pd.read_hdf('/content/drive/MyDrive/aes/t100/y_train.h5').values\n","y_train = to_categorical(y_train)\n","y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvhhWXII2ukf","executionInfo":{"status":"ok","timestamp":1681354467694,"user_tz":-540,"elapsed":633,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"cf22bb1a-cb62-4110-8f53-9bf7c5ee0925"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["y_test = pd.read_hdf('/content/drive/MyDrive/aes/t100/y_test.h5').values\n","y_test = to_categorical(y_test)\n","y_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5j1u_me2w68","executionInfo":{"status":"ok","timestamp":1681354468494,"user_tz":-540,"elapsed":802,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"91860055-520a-4e3c-ac22-a4cbe2ebd3ef"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from keras.backend import set_session\n","from keras.backend import clear_session\n","from keras.backend import get_session\n","import gc\n","def reset_keras():\n","    sess = get_session()\n","    clear_session()\n","    sess.close()\n","    sess = get_session()\n","\n","    try:\n","        del classifier # this is from global space - change this as you need\n","    except:\n","        pass\n","\n","    print(gc.collect()) # if it does something you should see a number as output\n","\n","    # use the same config as you used to create the session\n","    config = tf.compat.v1.ConfigProto()\n","    config.gpu_options.per_process_gpu_memory_fraction = 1\n","    config.gpu_options.visible_device_list = \"0\"\n","    set_session(tf.compat.v1.Session(config=config))"],"metadata":{"id":"tH_k8nyN7Guf","executionInfo":{"status":"ok","timestamp":1681354468495,"user_tz":-540,"elapsed":2,"user":{"displayName":"김민","userId":"01899801562476156168"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# units : 라벨의 갯수, input_dim : 특성의 갯수\n","from tensorflow import keras\n","model = Sequential([\n","    # Dropout(rate = 0.5),\n","    Dense(units = 77, input_dim = 70000 , activation = 'softmax')\n","])\n","\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n","\n","callback = keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,patience=10,verbose=1,restore_best_weights=True)"],"metadata":{"id":"z7LLpVe92yEv","executionInfo":{"status":"ok","timestamp":1681354471268,"user_tz":-540,"elapsed":2774,"user":{"displayName":"김민","userId":"01899801562476156168"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["reset_keras()\n","history = model.fit(x_train, y_train, batch_size = 512, epochs = 300, callbacks = [callback] ,validation_data = (x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwB3_QVw20WJ","executionInfo":{"status":"ok","timestamp":1681355356273,"user_tz":-540,"elapsed":885013,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"210924b2-07a7-4675-bd7b-083e0b107819"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["166\n","Epoch 1/300\n","110/110 [==============================] - 28s 209ms/step - loss: 2.0487 - acc: 0.5761 - val_loss: 1.2680 - val_acc: 0.7877\n","Epoch 2/300\n","110/110 [==============================] - 6s 51ms/step - loss: 1.0056 - acc: 0.8591 - val_loss: 0.8632 - val_acc: 0.8380\n","Epoch 3/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.7092 - acc: 0.9272 - val_loss: 0.6646 - val_acc: 0.9373\n","Epoch 4/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.5472 - acc: 0.9638 - val_loss: 0.5514 - val_acc: 0.9314\n","Epoch 5/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.4447 - acc: 0.9777 - val_loss: 0.4746 - val_acc: 0.9476\n","Epoch 6/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.3712 - acc: 0.9904 - val_loss: 0.4143 - val_acc: 0.9701\n","Epoch 7/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.3163 - acc: 0.9940 - val_loss: 0.3728 - val_acc: 0.9678\n","Epoch 8/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.2739 - acc: 0.9948 - val_loss: 0.3357 - val_acc: 0.9728\n","Epoch 9/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.2393 - acc: 0.9967 - val_loss: 0.3069 - val_acc: 0.9737\n","Epoch 10/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.2112 - acc: 0.9982 - val_loss: 0.2821 - val_acc: 0.9844\n","Epoch 11/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.1875 - acc: 0.9993 - val_loss: 0.2628 - val_acc: 0.9819\n","Epoch 12/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.1680 - acc: 0.9992 - val_loss: 0.2444 - val_acc: 0.9835\n","Epoch 13/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.1508 - acc: 0.9996 - val_loss: 0.2295 - val_acc: 0.9802\n","Epoch 14/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.1373 - acc: 0.9993 - val_loss: 0.2143 - val_acc: 0.9856\n","Epoch 15/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.1240 - acc: 0.9997 - val_loss: 0.2033 - val_acc: 0.9838\n","Epoch 16/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.1131 - acc: 0.9997 - val_loss: 0.1925 - val_acc: 0.9839\n","Epoch 17/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.1034 - acc: 0.9999 - val_loss: 0.1870 - val_acc: 0.9800\n","Epoch 18/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0947 - acc: 0.9997 - val_loss: 0.1745 - val_acc: 0.9863\n","Epoch 19/300\n","110/110 [==============================] - 6s 50ms/step - loss: 0.0869 - acc: 0.9999 - val_loss: 0.1662 - val_acc: 0.9882\n","Epoch 20/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0803 - acc: 0.9999 - val_loss: 0.1586 - val_acc: 0.9896\n","Epoch 21/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0740 - acc: 0.9999 - val_loss: 0.1523 - val_acc: 0.9897\n","Epoch 22/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0685 - acc: 1.0000 - val_loss: 0.1469 - val_acc: 0.9889\n","Epoch 23/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0637 - acc: 1.0000 - val_loss: 0.1406 - val_acc: 0.9900\n","Epoch 24/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0590 - acc: 1.0000 - val_loss: 0.1358 - val_acc: 0.9895\n","Epoch 25/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0551 - acc: 1.0000 - val_loss: 0.1311 - val_acc: 0.9891\n","Epoch 26/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0512 - acc: 1.0000 - val_loss: 0.1270 - val_acc: 0.9898\n","Epoch 27/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0477 - acc: 1.0000 - val_loss: 0.1221 - val_acc: 0.9902\n","Epoch 28/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0445 - acc: 1.0000 - val_loss: 0.1179 - val_acc: 0.9909\n","Epoch 29/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0417 - acc: 1.0000 - val_loss: 0.1144 - val_acc: 0.9904\n","Epoch 30/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0389 - acc: 1.0000 - val_loss: 0.1112 - val_acc: 0.9903\n","Epoch 31/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0365 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9903\n","Epoch 32/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.1047 - val_acc: 0.9907\n","Epoch 33/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.1020 - val_acc: 0.9915\n","Epoch 34/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0302 - acc: 1.0000 - val_loss: 0.0990 - val_acc: 0.9911\n","Epoch 35/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 0.9920\n","Epoch 36/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9909\n","Epoch 37/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0252 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9916\n","Epoch 38/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.0889 - val_acc: 0.9922\n","Epoch 39/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 0.9918\n","Epoch 40/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0854 - val_acc: 0.9917\n","Epoch 41/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9921\n","Epoch 42/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0812 - val_acc: 0.9923\n","Epoch 43/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9918\n","Epoch 44/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0775 - val_acc: 0.9924\n","Epoch 45/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 0.9922\n","Epoch 46/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0744 - val_acc: 0.9922\n","Epoch 47/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0729 - val_acc: 0.9917\n","Epoch 48/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0716 - val_acc: 0.9917\n","Epoch 49/300\n","110/110 [==============================] - 6s 50ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0702 - val_acc: 0.9915\n","Epoch 50/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0688 - val_acc: 0.9919\n","Epoch 51/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.9911\n","Epoch 52/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0670 - val_acc: 0.9913\n","Epoch 53/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 0.9919\n","Epoch 54/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 0.9917\n","Epoch 55/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9923\n","Epoch 56/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 0.9920\n","Epoch 57/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 0.9921\n","Epoch 58/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9918\n","Epoch 59/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 0.9922\n","Epoch 60/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9925\n","Epoch 61/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9921\n","Epoch 62/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 0.9920\n","Epoch 63/300\n","110/110 [==============================] - 6s 50ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9919\n","Epoch 64/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9919\n","Epoch 65/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 0.9926\n","Epoch 66/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9919\n","Epoch 67/300\n","110/110 [==============================] - 6s 50ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9922\n","Epoch 68/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9926\n","Epoch 69/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 0.9923\n","Epoch 70/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9924\n","Epoch 71/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0486 - val_acc: 0.9920\n","Epoch 72/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9926\n","Epoch 73/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9920\n","Epoch 74/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9926\n","Epoch 75/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0460 - val_acc: 0.9925\n","Epoch 76/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0452 - val_acc: 0.9926\n","Epoch 77/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0448 - val_acc: 0.9919\n","Epoch 78/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 0.9924\n","Epoch 79/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 0.9922\n","Epoch 80/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 0.9927\n","Epoch 81/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0428 - val_acc: 0.9925\n","Epoch 82/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 0.9925\n","Epoch 83/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0417 - val_acc: 0.9925\n","Epoch 84/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0412 - val_acc: 0.9927\n","Epoch 85/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0407 - val_acc: 0.9927\n","Epoch 86/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9927\n","Epoch 87/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0396 - val_acc: 0.9924\n","Epoch 88/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0392 - val_acc: 0.9924\n","Epoch 89/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0389 - val_acc: 0.9925\n","Epoch 90/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0389 - val_acc: 0.9924\n","Epoch 91/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0385 - val_acc: 0.9917\n","Epoch 92/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0383 - val_acc: 0.9919\n","Epoch 93/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0378 - val_acc: 0.9919\n","Epoch 94/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 0.9927\n","Epoch 95/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0365 - val_acc: 0.9927\n","Epoch 96/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0362 - val_acc: 0.9927\n","Epoch 97/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0359 - val_acc: 0.9928\n","Epoch 98/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0356 - val_acc: 0.9924\n","Epoch 99/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0350 - val_acc: 0.9927\n","Epoch 100/300\n","110/110 [==============================] - 6s 53ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0346 - val_acc: 0.9926\n","Epoch 101/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0343 - val_acc: 0.9927\n","Epoch 102/300\n","110/110 [==============================] - 6s 52ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0340 - val_acc: 0.9929\n","Epoch 103/300\n","110/110 [==============================] - 6s 51ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0338 - val_acc: 0.9924\n","Epoch 104/300\n","110/110 [==============================] - 6s 53ms/step - loss: 9.8805e-04 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 0.9931\n","Epoch 105/300\n","110/110 [==============================] - 6s 51ms/step - loss: 9.4832e-04 - acc: 1.0000 - val_loss: 0.0337 - val_acc: 0.9920\n","Epoch 106/300\n","110/110 [==============================] - 6s 52ms/step - loss: 9.1014e-04 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 0.9928\n","Epoch 107/300\n","110/110 [==============================] - 6s 51ms/step - loss: 8.7298e-04 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 0.9922\n","Epoch 108/300\n","110/110 [==============================] - 6s 50ms/step - loss: 8.4076e-04 - acc: 1.0000 - val_loss: 0.0324 - val_acc: 0.9928\n","Epoch 109/300\n","110/110 [==============================] - 6s 52ms/step - loss: 8.1036e-04 - acc: 1.0000 - val_loss: 0.0321 - val_acc: 0.9926\n","Epoch 110/300\n","110/110 [==============================] - 6s 51ms/step - loss: 7.8018e-04 - acc: 1.0000 - val_loss: 0.0320 - val_acc: 0.9925\n","Epoch 111/300\n","110/110 [==============================] - 6s 51ms/step - loss: 7.5190e-04 - acc: 1.0000 - val_loss: 0.0317 - val_acc: 0.9928\n","Epoch 112/300\n","110/110 [==============================] - 6s 51ms/step - loss: 7.2327e-04 - acc: 1.0000 - val_loss: 0.0312 - val_acc: 0.9928\n","Epoch 113/300\n","110/110 [==============================] - 6s 51ms/step - loss: 6.9704e-04 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9929\n","Epoch 114/300\n","110/110 [==============================] - 6s 52ms/step - loss: 6.7097e-04 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 0.9931\n","Epoch 115/300\n","110/110 [==============================] - 6s 51ms/step - loss: 6.4782e-04 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 0.9931\n","Epoch 116/300\n","110/110 [==============================] - 6s 53ms/step - loss: 6.2487e-04 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 0.9928\n","Epoch 117/300\n","110/110 [==============================] - 6s 51ms/step - loss: 6.0565e-04 - acc: 1.0000 - val_loss: 0.0301 - val_acc: 0.9924\n","Epoch 118/300\n","110/110 [==============================] - 6s 51ms/step - loss: 5.8447e-04 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 0.9932\n","Epoch 119/300\n","110/110 [==============================] - 6s 52ms/step - loss: 5.6320e-04 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 0.9928\n","Epoch 120/300\n","110/110 [==============================] - 6s 51ms/step - loss: 5.4588e-04 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9932\n","Epoch 121/300\n","110/110 [==============================] - 6s 51ms/step - loss: 5.2468e-04 - acc: 1.0000 - val_loss: 0.0290 - val_acc: 0.9930\n","Epoch 122/300\n","110/110 [==============================] - 6s 51ms/step - loss: 5.0563e-04 - acc: 1.0000 - val_loss: 0.0290 - val_acc: 0.9931\n","Epoch 123/300\n","110/110 [==============================] - 6s 53ms/step - loss: 4.8991e-04 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 0.9927\n","Epoch 124/300\n","110/110 [==============================] - 6s 52ms/step - loss: 4.7519e-04 - acc: 1.0000 - val_loss: 0.0290 - val_acc: 0.9927\n","Epoch 125/300\n","110/110 [==============================] - 6s 51ms/step - loss: 4.5903e-04 - acc: 1.0000 - val_loss: 0.0287 - val_acc: 0.9926\n","Epoch 126/300\n","110/110 [==============================] - 6s 52ms/step - loss: 4.4383e-04 - acc: 1.0000 - val_loss: 0.0286 - val_acc: 0.9927\n","Epoch 127/300\n","110/110 [==============================] - 6s 52ms/step - loss: 4.2989e-04 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9927\n","Epoch 128/300\n","110/110 [==============================] - 6s 52ms/step - loss: 4.1650e-04 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9932\n","Epoch 129/300\n","110/110 [==============================] - 6s 52ms/step - loss: 4.0240e-04 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 0.9926\n","Epoch 130/300\n","110/110 [==============================] - 6s 51ms/step - loss: 3.9217e-04 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 0.9931\n","Epoch 131/300\n","110/110 [==============================] - 6s 51ms/step - loss: 3.8017e-04 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 0.9932\n","Epoch 132/300\n","110/110 [==============================] - 6s 52ms/step - loss: 3.6706e-04 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 0.9931\n","Epoch 133/300\n","110/110 [==============================] - 6s 52ms/step - loss: 3.5576e-04 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 0.9930\n","Epoch 134/300\n","110/110 [==============================] - 6s 50ms/step - loss: 3.4702e-04 - acc: 1.0000 - val_loss: 0.0270 - val_acc: 0.9932\n","Epoch 135/300\n","110/110 [==============================] - 6s 53ms/step - loss: 3.3495e-04 - acc: 1.0000 - val_loss: 0.0272 - val_acc: 0.9926\n","Epoch 136/300\n","110/110 [==============================] - 6s 51ms/step - loss: 3.2555e-04 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 0.9932\n","Epoch 137/300\n","110/110 [==============================] - 6s 51ms/step - loss: 3.1759e-04 - acc: 1.0000 - val_loss: 0.0263 - val_acc: 0.9934\n","Epoch 138/300\n","110/110 [==============================] - 6s 51ms/step - loss: 3.0904e-04 - acc: 1.0000 - val_loss: 0.0261 - val_acc: 0.9935\n","Epoch 139/300\n","110/110 [==============================] - 6s 51ms/step - loss: 2.9901e-04 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 0.9929\n","Epoch 140/300\n","110/110 [==============================] - 6s 50ms/step - loss: 2.9117e-04 - acc: 1.0000 - val_loss: 0.0261 - val_acc: 0.9929\n","Epoch 141/300\n","110/110 [==============================] - 6s 53ms/step - loss: 2.8215e-04 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 0.9931\n","Epoch 142/300\n","110/110 [==============================] - 6s 52ms/step - loss: 2.7642e-04 - acc: 1.0000 - val_loss: 0.0257 - val_acc: 0.9929\n","Epoch 143/300\n","110/110 [==============================] - 6s 51ms/step - loss: 2.6832e-04 - acc: 1.0000 - val_loss: 0.0256 - val_acc: 0.9933\n","Epoch 144/300\n","110/110 [==============================] - 6s 52ms/step - loss: 2.6105e-04 - acc: 1.0000 - val_loss: 0.0256 - val_acc: 0.9929\n","Epoch 145/300\n","109/110 [============================>.] - ETA: 0s - loss: 2.5444e-04 - acc: 1.0000Restoring model weights from the end of the best epoch: 135.\n","110/110 [==============================] - 6s 52ms/step - loss: 2.5446e-04 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 0.9930\n","Epoch 145: early stopping\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","y_pred = model.predict(x_test)\n","print(classification_report(np.argmax(y_test,axis=1),np.argmax(y_pred,axis=1)))"],"metadata":{"id":"zZSdR1Lq_Zwk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681355376255,"user_tz":-540,"elapsed":19991,"user":{"displayName":"김민","userId":"01899801562476156168"}},"outputId":"01983da1-f1eb-467e-a6d7-339768a57666"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["749/749 [==============================] - 3s 3ms/step\n","              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       315\n","           2       0.98      0.98      0.98       315\n","           3       1.00      1.00      1.00       315\n","           4       1.00      1.00      1.00       315\n","           5       1.00      1.00      1.00       315\n","           6       1.00      1.00      1.00       316\n","           7       1.00      1.00      1.00       315\n","           8       1.00      1.00      1.00       315\n","           9       0.98      0.98      0.98       315\n","          10       1.00      1.00      1.00       315\n","          11       1.00      1.00      1.00       315\n","          12       1.00      1.00      1.00       315\n","          13       1.00      1.00      1.00       315\n","          14       1.00      1.00      1.00       315\n","          15       1.00      1.00      1.00       315\n","          16       1.00      1.00      1.00       315\n","          17       1.00      1.00      1.00       315\n","          18       1.00      1.00      1.00       315\n","          19       1.00      1.00      1.00       315\n","          20       1.00      1.00      1.00       315\n","          21       1.00      1.00      1.00       315\n","          22       1.00      1.00      1.00       315\n","          23       1.00      1.00      1.00       315\n","          24       1.00      1.00      1.00       315\n","          25       1.00      1.00      1.00       315\n","          26       1.00      1.00      1.00       315\n","          27       1.00      1.00      1.00       315\n","          28       1.00      1.00      1.00       315\n","          29       1.00      1.00      1.00       315\n","          30       1.00      1.00      1.00       315\n","          31       1.00      1.00      1.00       315\n","          32       1.00      1.00      1.00       315\n","          33       1.00      1.00      1.00       315\n","          34       1.00      1.00      1.00       315\n","          35       1.00      1.00      1.00       316\n","          36       1.00      1.00      1.00       316\n","          37       1.00      0.99      0.99       316\n","          38       1.00      1.00      1.00       315\n","          39       1.00      1.00      1.00       315\n","          40       1.00      1.00      1.00       315\n","          41       0.99      1.00      0.99       315\n","          42       1.00      1.00      1.00       315\n","          43       0.98      0.97      0.98       315\n","          44       1.00      0.99      1.00       315\n","          45       1.00      1.00      1.00       315\n","          46       1.00      1.00      1.00       315\n","          47       0.97      0.98      0.97       315\n","          48       1.00      0.99      0.99       317\n","          49       1.00      1.00      1.00       316\n","          50       0.94      0.94      0.94       315\n","          51       1.00      1.00      1.00       316\n","          52       0.98      0.99      0.99       312\n","          53       1.00      1.00      1.00       315\n","          54       0.94      0.94      0.94       315\n","          55       0.98      0.97      0.97       315\n","          56       0.99      1.00      0.99       315\n","          57       1.00      1.00      1.00       315\n","          58       0.96      0.83      0.89       315\n","          59       0.85      0.96      0.90       315\n","          60       0.96      0.97      0.97       315\n","          61       1.00      1.00      1.00       315\n","          62       1.00      1.00      1.00       315\n","          63       1.00      1.00      1.00       316\n","          64       1.00      1.00      1.00       315\n","          65       1.00      1.00      1.00       315\n","          66       1.00      1.00      1.00       315\n","          67       1.00      1.00      1.00       315\n","          68       1.00      1.00      1.00       315\n","          69       1.00      1.00      1.00       315\n","          70       1.00      1.00      1.00       319\n","          71       1.00      1.00      1.00       315\n","          72       1.00      1.00      1.00       315\n","          73       1.00      1.00      1.00       315\n","          74       1.00      1.00      1.00       315\n","          75       1.00      1.00      1.00       315\n","          76       1.00      1.00      1.00       315\n","\n","    accuracy                           0.99     23950\n","   macro avg       0.99      0.99      0.99     23950\n","weighted avg       0.99      0.99      0.99     23950\n","\n"]}]}]}